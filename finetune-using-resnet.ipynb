{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2739dcd-369f-4f4a-b36d-be41b8a94761",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Epoch [1/20], Step [10/18], Loss: 0.5703415155410767\n",
      "Epoch [2/20], Step [10/18], Loss: 0.43279818892478944\n",
      "Epoch [3/20], Step [10/18], Loss: 0.4207752853631973\n",
      "Epoch [4/20], Step [10/18], Loss: 0.404787203669548\n",
      "Epoch [5/20], Step [10/18], Loss: 0.4122891366481781\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch [6/20], Step [10/18], Loss: 0.3784020394086838\n",
      "Epoch [7/20], Step [10/18], Loss: 0.35817195773124694\n",
      "Epoch [8/20], Step [10/18], Loss: 0.3513214588165283\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch [9/20], Step [10/18], Loss: 0.39106438457965853\n",
      "Epoch [10/20], Step [10/18], Loss: 0.41537892520427705\n",
      "Epoch [11/20], Step [10/18], Loss: 0.3721770316362381\n",
      "Epoch [12/20], Step [10/18], Loss: 0.4093081772327423\n",
      "Epoch [13/20], Step [10/18], Loss: 0.4336109280586243\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch [14/20], Step [10/18], Loss: 0.38721477389335635\n",
      "Epoch [15/20], Step [10/18], Loss: 0.4101632982492447\n",
      "Epoch [16/20], Step [10/18], Loss: 0.44409090280532837\n",
      "Epoch [17/20], Step [10/18], Loss: 0.34628221988677976\n",
      "Epoch [18/20], Step [10/18], Loss: 0.3583570122718811\n",
      "Epoch [19/20], Step [10/18], Loss: 0.42242548763751986\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch [20/20], Step [10/18], Loss: 0.4452923506498337\n",
      "Test Accuracy: 82.00%\n",
      "Precision: 0.86\n",
      "Recall: 0.83\n",
      "F1 Score: 0.84\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "from torch.utils.tensorboard import SummaryWriter  # For visualization\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score  # For evaluation metrics\n",
    "\n",
    "\n",
    "# Initialize TensorBoard\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 20  # Increased from 10 to 20\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4  # L2 regularization\n",
    "\n",
    "# Define transformations with data augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 pixels\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flipping\n",
    "    transforms.RandomRotation(10),  # Random rotation by 10 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Randomly adjust brightness, contrast, and saturation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n",
    "])\n",
    "\n",
    "# Use a simpler transform for validation/test dataset without augmentation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 pixels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization\n",
    "])\n",
    "\n",
    "# Load COCO dataset with augmentation for training set\n",
    "train_dataset = CocoDetection(root='./dataset/ZiggoPortStatus-2/train', annFile='./dataset/ZiggoPortStatus-2/train/_annotations.coco.json', transform=train_transform)\n",
    "# No augmentation for test dataset\n",
    "test_dataset = CocoDetection(root='./dataset/ZiggoPortStatus-2/test', annFile='./dataset/ZiggoPortStatus-2/test/_annotations.coco.json', transform=test_transform)\n",
    "\n",
    "# Load category information and define class_names\n",
    "with open('./dataset/ZiggoPortStatus-2/train/_annotations.coco.json', 'r') as f:\n",
    "    coco_info = json.load(f)\n",
    "class_names = {cat['id']: cat['name'] for cat in coco_info['categories']}\n",
    "\n",
    "# Create a mapping from COCO category IDs to sequential indices\n",
    "cat_id_to_seq_id = {cat_id: idx for idx, cat_id in enumerate(sorted(class_names.keys()))}\n",
    "\n",
    "# Model setup\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, len(cat_id_to_seq_id))\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer with weight decay for L2 regularization\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Scheduler for adjusting learning rate\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.1, verbose=True)\n",
    "\n",
    "\n",
    "# Custom collate function for handling COCO annotations\n",
    "def custom_collate_fn(batch):\n",
    "    images, annotations = zip(*batch)\n",
    "    labels_one_hot = torch.zeros((len(annotations), len(cat_id_to_seq_id)))\n",
    "    for idx, anns in enumerate(annotations):\n",
    "        for ann in anns:\n",
    "            seq_id = cat_id_to_seq_id[ann['category_id']]\n",
    "            labels_one_hot[idx, seq_id] = 1\n",
    "    return torch.stack(images), labels_one_hot\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels_one_hot) in enumerate(train_loader):\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels_one_hot)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (i + 1) % 10 == 0:  # Log every 10 batches\n",
    "            current_loss = running_loss / 10\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {current_loss}')\n",
    "            writer.add_scalar('training loss', current_loss, epoch * len(train_loader) + i)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Adjust learning rate based on the epoch loss\n",
    "    scheduler.step(running_loss)\n",
    "\n",
    "writer.close()  # Close the TensorBoard\n",
    "\n",
    "# Evaluation with additional metrics\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels_one_hot in test_loader:\n",
    "        outputs = model(images)\n",
    "        predicted_labels = torch.sigmoid(outputs) > 0.5\n",
    "        y_true.extend(labels_one_hot.cpu().numpy())\n",
    "        y_pred.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "# Convert predictions and true labels to binary for evaluation metrics\n",
    "y_true = np.array(y_true).flatten()\n",
    "y_pred = np.array(y_pred).flatten()\n",
    "\n",
    "accuracy = (y_true == y_pred).mean()\n",
    "precision = precision_score(y_true, y_pred, average='binary')\n",
    "recall = recall_score(y_true, y_pred, average='binary')\n",
    "f1 = f1_score(y_true, y_pred, average='binary')\n",
    "\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18544efc-b82e-4ae3-8021-cc479c2b8ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
